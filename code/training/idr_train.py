import os
from datetime import datetime
from pyhocon import ConfigFactory
import sys
import torch

import utils.general as utils
import utils.plots as plt
from datetime import datetime
import trimesh
# import pymesh
from numpy.linalg import norm
import numpy as np
import open3d as o3d
import cv2
import time
from utils.general import sync_print
from tensorboardX import SummaryWriter
from largesteps.optimize import AdamUniform
from largesteps.parameterize import to_differential
from largesteps.geometry import compute_matrix
from largesteps.geometry import compute_matrix, laplacian_uniform

class Meshlabserver:
    def __init__(self):
        # script generated by meshlab-2020.04
        self.meshlab_remesh_srcipt = """
        <!DOCTYPE FilterScript>
        <FilterScript>
         <filter name="Remeshing: Isotropic Explicit Remeshing">
          <Param value="3" isxmlparam="0" name="Iterations" type="RichInt" description="Iterations" tooltip="Number of iterations of the remeshing operations to repeat on the mesh."/>
          <Param value="false" isxmlparam="0" name="Adaptive" type="RichBool" description="Adaptive remeshing" tooltip="Toggles adaptive isotropic remeshing."/>
          <Param value="false" isxmlparam="0" name="SelectedOnly" type="RichBool" description="Remesh only selected faces" tooltip="If checked the remeshing operations will be applied only to the selected faces."/>
          <Param value="{}" isxmlparam="0" name="TargetLen" type="RichAbsPerc" description="Target Length" min="0" max="214.384" tooltip="Sets the target length for the remeshed mesh edges."/>
          <Param value="180" isxmlparam="0" name="FeatureDeg" type="RichFloat" description="Crease Angle" tooltip="Minimum angle between faces of the original to consider the shared edge as a feature to be preserved."/>
          <Param value="true" isxmlparam="0" name="CheckSurfDist" type="RichBool" description="Check Surface Distance" tooltip="If toggled each local operation must deviate from original mesh by [Max. surface distance]"/>
          <Param value="1" isxmlparam="0" name="MaxSurfDist" type="RichAbsPerc" description="Max. Surface Distance" min="0" max="214.384" tooltip="Maximal surface deviation allowed for each local operation"/>
          <Param value="true" isxmlparam="0" name="SplitFlag" type="RichBool" description="Refine Step" tooltip="If checked the remeshing operations will include a refine step."/>
          <Param value="true" isxmlparam="0" name="CollapseFlag" type="RichBool" description="Collapse Step" tooltip="If checked the remeshing operations will include a collapse step."/>
          <Param value="true" isxmlparam="0" name="SwapFlag" type="RichBool" description="Edge-Swap Step" tooltip="If checked the remeshing operations will include a edge-swap step, aimed at improving the vertex valence of the resulting mesh."/>
          <Param value="true" isxmlparam="0" name="SmoothFlag" type="RichBool" description="Smooth Step" tooltip="If checked the remeshing operations will include a smoothing step, aimed at relaxing the vertex positions in a Laplacian sense."/>
          <Param value="true" isxmlparam="0" name="ReprojectFlag" type="RichBool" description="Reproject Step" tooltip="If checked the remeshing operations will include a step to reproject the mesh vertices on the original surface."/>
         </filter>
        </FilterScript>
        """
        pid = str(os.getpid())
        path = '/home/2TB/xjm/transparentobjectrecon/temp/'
        utils.mkdir_ifnotexists(path)
        self.ply_path = f"{path}/temp_{pid}.ply"
        self.remeshply_path = f"{path}/remesh_{pid}.ply"
        self.script_path = f"{path}/script_{pid}.mlx"
        
        meshlabserver_cmd = "/home/2TB/xjm/MeshLabServer2020.04-linux.AppImage"
        
        self.cmd = meshlabserver_cmd + \
            ' -i ' + self.ply_path + \
            ' -o ' + self.remeshply_path + \
            ' -s ' + self.script_path
        self.cmd = self.cmd# + " 1>/dev/null 2>&1"
        print(self.cmd)
        # input()
        
    def remesh(self, mesh, remesh_len):

        meshlab_remesh_srcipt = self.meshlab_remesh_srcipt.format(remesh_len)
        with open(self.script_path, 'w') as script_file:
            script_file.write(meshlab_remesh_srcipt)
        mesh.export(self.ply_path)
        assert(os.system(self.cmd) == 0)
        # scene.update_mesh(self.remeshply_path) 
        mesh = trimesh.load(self.remeshply_path, process=False)
        return mesh

        # os.system('rm '+tmpply)
        # os.system('rm '+remeshply)     
        # os.system('rm '+self.script_path)

def limit_hook(grad):
    # max = 0.008
    max = 0.0008
    if torch.isnan(grad).any():
        print("nan in grad")
        exit(1)
    grad[torch.isnan(grad)] = 0
    grad[grad>max]=max
    grad[grad<-max]=-max
    return grad

class IDRTrainRunner():
    def __init__(self, **kwargs):
        torch.set_default_dtype(torch.float32)
        torch.set_num_threads(1)

        self.conf = ConfigFactory.parse_file(kwargs['conf'])
        self.data_dir = self.conf.get('model.data_dir')
        self.batch_size = kwargs['batch_size']
        self.vsa_num = kwargs['vsa_num']
        self.nepochs = kwargs['nepochs']
        self.GPU_INDEX =kwargs['gpu_index'] 
        self.exps_folder_name = kwargs['exps_folder_name']

        self.train_explicit_vertices_original = kwargs['train_explicit_vertices_original']
        self.train_explicit_vertices_large_step = kwargs['train_explicit_vertices_large_step']
        self.train_idr_sdf_single_mlp = kwargs['train_idr_sdf_single_mlp']
        self.train_mesh_sdf_single_mlp = kwargs['train_mesh_sdf_single_mlp']
        self.train_displacements_single_mlp = kwargs['train_displacements_single_mlp']
        self.train_displacements_multi_mlp = kwargs['train_displacements_multi_mlp']

        self.use_our_corr = kwargs['use_our_corr']
        self.use_DRT_corr = kwargs['use_DRT_corr']
        self.enable_remeshing = kwargs['enable_remeshing']

        print('----------------------------------')
        print("1. train_explicit_vertices_original: {0}".format(self.train_explicit_vertices_original))
        print("2. train_explicit_vertices_large_step: {0}".format(self.train_explicit_vertices_large_step))
        print("3. train_idr_sdf_single_mlp: {0}".format(self.train_idr_sdf_single_mlp))
        print("4. train_mesh_sdf_single_mlp: {0}".format(self.train_mesh_sdf_single_mlp))
        print("5. train_displacements_single_mlp: {0}".format(self.train_displacements_single_mlp))
        print("6. train_displacements_multi_mlp: {0}".format(self.train_displacements_multi_mlp))

        print("use_our_corr: {0}".format(self.use_our_corr))
        print("use_DRT_corr: {0}".format(self.use_DRT_corr))
        print("enable_remeshing: {0}".format(self.enable_remeshing))
        print('----------------------------------')
        # input()

        if kwargs['expname'] != '':
            self.expname = self.conf.get_string('train.expname') + '_' + kwargs['expname']
        else:
            self.expname = self.conf.get_string('train.expname')
        if kwargs['is_continue'] and kwargs['timestamp'] == 'latest':
            if os.path.exists(os.path.join('../',kwargs['exps_folder_name'],self.expname)):
                timestamps = os.listdir(os.path.join('../',kwargs['exps_folder_name'],self.expname))
                if (len(timestamps)) == 0:
                    self.is_continue = False
                    timestamp = None
                else:
                    timestamp = sorted(timestamps)[-1]
                    self.is_continue = True
            else:
                self.is_continue = False
                timestamp = None
        else:
            timestamp = kwargs['timestamp']
            self.is_continue = kwargs['is_continue']

        utils.mkdir_ifnotexists(os.path.join('../',self.exps_folder_name))
        self.expdir = os.path.join('../', self.exps_folder_name, self.expname)
        utils.mkdir_ifnotexists(self.expdir)
        self.timestamp = '{:%Y_%m_%d_%H_%M_%S}'.format(datetime.now())
        utils.mkdir_ifnotexists(os.path.join(self.expdir, self.timestamp))

        self.plots_dir = os.path.join(self.expdir, self.timestamp, 'plots')
        utils.mkdir_ifnotexists(self.plots_dir)

        # create checkpoints dirs
        self.checkpoints_path = os.path.join(self.expdir, self.timestamp, 'checkpoints')
        utils.mkdir_ifnotexists(self.checkpoints_path)
        self.model_params_subdir = "ModelParameters"
        self.optimizer_params_subdir = "OptimizerParameters"
        self.scheduler_params_subdir = "SchedulerParameters"

        utils.mkdir_ifnotexists(os.path.join(self.checkpoints_path, self.model_params_subdir))
        utils.mkdir_ifnotexists(os.path.join(self.checkpoints_path, self.optimizer_params_subdir))
        utils.mkdir_ifnotexists(os.path.join(self.checkpoints_path, self.scheduler_params_subdir))

        os.system("""cp -r {0} "{1}" """.format(kwargs['conf'], os.path.join(self.expdir, self.timestamp, 'runconf.conf')))

        if (not self.GPU_INDEX == 'ignore'):
            os.environ["CUDA_VISIBLE_DEVICES"] = '{0}'.format(self.GPU_INDEX)

        print('shell command : {0}'.format(' '.join(sys.argv)))

        print('Loading data ...')
        dataset_conf = self.conf.get_config('dataset')
        self.train_dataset = utils.get_class(self.conf.get_string('train.dataset_class'))(self.use_our_corr,
                                                                                          self.use_DRT_corr,
                                                                                          (self.train_idr_sdf_single_mlp or self.train_idr_sdf_single_mlp),
                                                                                          **dataset_conf)
        print('Finish loading data ...')

        self.train_dataloader = torch.utils.data.DataLoader(self.train_dataset,
                                                            batch_size=self.batch_size,
                                                            shuffle=True,
                                                            pin_memory=True,
                                                            collate_fn=self.train_dataset.collate_fn,
                                                            num_workers=16)
        self.plot_dataloader = torch.utils.data.DataLoader(self.train_dataset,
                                                           batch_size=self.conf.get_int('plot.plot_nimgs'),
                                                           shuffle=False,
                                                           collate_fn=self.train_dataset.collate_fn)
        self.cluster_vertex_start_end = None
        self.geodesic_indices = None
        self.geodesic_weights = None
        self.cluster_vertex_start_end = np.load(self.data_dir + 'vsa_' + str(self.vsa_num) + '/cluster_vertex_start_end.npy')
        geodesic_indices = np.load(self.data_dir + 'vsa_' + str(self.vsa_num) + '/geodesic_indices_knn_40.npy')
        geodesic_weights = np.load(self.data_dir + 'vsa_' + str(self.vsa_num) + '/geodesic_weights_knn_40.npy')
        self.geodesic_indices = torch.from_numpy(geodesic_indices).long().cuda()
        self.geodesic_weights = torch.from_numpy(geodesic_weights).float().cuda()

        self.model = utils.get_class(self.conf.get_string('train.model_class'))(self.cluster_vertex_start_end, self.geodesic_indices, self.geodesic_weights, conf=self.conf.get_config('model'))
        if torch.cuda.is_available():
            self.model.cuda()

        if self.model.sdf_mesh is None:
            self.model.sdf_mesh = trimesh.load(self.data_dir + 'vsa_' + str(self.vsa_num) + '/mask_loss_mesh_vsa_no_color.obj', use_embree=True, process=False)
            # print(self.data_dir + 'vsa_' + str(self.vsa_num) + '/mask_loss_mesh_vsa_no_color.obj')
            # print(self.model.sdf_mesh.vertices.shape)
            self.model.mesh_vertices_tensor = torch.from_numpy(np.array(self.model.sdf_mesh.vertices)).float().cuda() 

        self.loss = utils.get_class(self.conf.get_string('train.loss_class'))(self.train_explicit_vertices_large_step, 
                                                                              (self.train_mesh_sdf_single_mlp or self.train_idr_sdf_single_mlp), 
                                                                              self.use_our_corr, 
                                                                              self.use_DRT_corr, 
                                                                              **self.conf.get_config('loss'))

        self.lr = self.conf.get_float('train.learning_rate')
        self.optimizer = None 
        self.scheduler = None
        self.optimizer_for_init = None 
        if self.train_explicit_vertices_original:
            self.explicit_vertices_tensor = self.model.mesh_vertices_tensor.clone().detach()
            self.explicit_vertices_tensor = self.explicit_vertices_tensor.cuda().requires_grad_()
            # self.explicit_vertices_tensor.register_hook(limit_hook)
            self.optimizer = torch.optim.Adam([self.explicit_vertices_tensor], lr=self.lr)
        elif self.train_explicit_vertices_large_step:
            self.explicit_vertices_tensor = self.model.mesh_vertices_tensor.clone()
            self.mesh_faces_tensor = torch.from_numpy(self.model.sdf_mesh.faces).int().cuda().detach()
            my_lambda = 10.0
            self.M = compute_matrix(self.explicit_vertices_tensor, self.mesh_faces_tensor, my_lambda)
            # Parameterize
            self.explicit_vertices_tensor_u = to_differential(self.M, self.explicit_vertices_tensor)
            self.explicit_vertices_tensor_u = self.explicit_vertices_tensor_u.requires_grad_()
            step_size = 2e-3 # Step size
            self.optimizer = AdamUniform([self.explicit_vertices_tensor_u], step_size)
        elif self.train_mesh_sdf_single_mlp:
            self.optimizer = torch.optim.Adam(self.model.implicit_network.parameters(), lr=1.0e-5)
        elif self.train_idr_sdf_single_mlp:
            self.optimizer = torch.optim.Adam(self.model.implicit_network.parameters(), lr=1.0e-5)
        elif self.train_displacements_multi_mlp:
            self.optimizer = torch.optim.Adam(self.model.multi_displacement_network.parameters(), lr=self.lr)
            # for p in self.model.multi_displacement_network.parameters():
            #     p.register_hook(limit_hook)
            # self.model.multi_displacement_network.parameters().register_hook(limit_hook)
            self.optimizer_for_init = torch.optim.Adam(self.model.multi_displacement_network.parameters(), lr=4.0e-5)
            self.optimizer = torch.optim.Adam(self.model.multi_displacement_network.parameters(), lr=self.lr)
        elif self.train_displacements_single_mlp:
            self.optimizer = torch.optim.Adam(self.model.displacement_network.parameters(), lr=self.lr)
            self.optimizer_for_init = torch.optim.Adam(self.model.displacement_network.parameters(), lr=1.0e-4)
        else:
            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)

        self.material_ior = torch.tensor([1.52]).cuda().float()
        self.material_ior = self.material_ior.requires_grad_()
        self.optimizer_ior = torch.optim.Adam([self.material_ior], lr=1.0e-9)
        # self.optimizer_ior = torch.optim.LBFGS([self.material_ior], lr=1.0e-1)

        if not self.train_explicit_vertices_large_step:
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=20)
        else:
            self.no_use_optimizer = torch.optim.Adam(self.model.implicit_network.parameters(), lr=self.lr)
            self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.no_use_optimizer, T_max=20)

        self.start_epoch = 0
        if self.is_continue:
            old_checkpnts_dir = os.path.join(self.expdir, timestamp, 'checkpoints')

            saved_model_state = torch.load(
                os.path.join(old_checkpnts_dir, 'ModelParameters', str(kwargs['checkpoint']) + ".pth"))

            if self.train_mesh_sdf_single_mlp or self.train_idr_sdf_single_mlp:
                for name, sub_module in self.model.named_children():
                    sub_module_dict = {}
                    for key in saved_model_state["model_state_dict"].keys():
                        if name in key and name == 'implicit_network':
                            new_key = '.'.join(key.split('.')[1:])
                            sub_module_dict[new_key] =  saved_model_state["model_state_dict"][key]
                    if len(sub_module_dict) > 0:
                        sub_module.load_state_dict(sub_module_dict)
            else:
                for name, sub_module in self.model.named_children():
                    sub_module_dict = {}
                    for key in saved_model_state["model_state_dict"].keys():
                        if name in key and name != 'displacement_network':
                            new_key = '.'.join(key.split('.')[1:])
                            sub_module_dict[new_key] =  saved_model_state["model_state_dict"][key]
                    if len(sub_module_dict) > 0:
                        sub_module.load_state_dict(sub_module_dict)
  
            self.start_epoch = saved_model_state['epoch']

        self.num_pixels = self.conf.get_int('train.num_pixels')
        self.total_pixels = self.train_dataset.total_pixels
        self.img_res = self.train_dataset.img_res
        self.n_batches = len(self.train_dataloader)
        self.plot_freq = self.conf.get_int('train.plot_freq')
        self.plot_conf = self.conf.get_config('plot')

        self.alpha_milestones = self.conf.get_list('train.alpha_milestones', default=[])
        self.alpha_factor = self.conf.get_float('train.alpha_factor', default=0.0)
        for acc in self.alpha_milestones:
            if self.start_epoch > acc:
                self.loss.alpha = self.loss.alpha * self.alpha_factor
             
    def save_checkpoints(self, epoch):
        torch.save(
            {"epoch": epoch, "model_state_dict": self.model.state_dict()},
            os.path.join(self.checkpoints_path, self.model_params_subdir, str(epoch) + ".pth"))
        torch.save(
            {"epoch": epoch, "model_state_dict": self.model.state_dict()},
            os.path.join(self.checkpoints_path, self.model_params_subdir, "latest.pth"))

        torch.save(
            {"epoch": epoch, "optimizer_state_dict": self.optimizer.state_dict()},
            os.path.join(self.checkpoints_path, self.optimizer_params_subdir, str(epoch) + ".pth"))
        torch.save(
            {"epoch": epoch, "optimizer_state_dict": self.optimizer.state_dict()},
            os.path.join(self.checkpoints_path, self.optimizer_params_subdir, "latest.pth"))

        torch.save(
            {"epoch": epoch, "scheduler_state_dict": self.scheduler.state_dict()},
            os.path.join(self.checkpoints_path, self.scheduler_params_subdir, str(epoch) + ".pth"))
        torch.save(
            {"epoch": epoch, "scheduler_state_dict": self.scheduler.state_dict()},
            os.path.join(self.checkpoints_path, self.scheduler_params_subdir, "latest.pth"))
        
    def interp_L(self, start, end, it, Pass):
        assert it <= Pass-1
        step = (end - start)/(Pass-1)
        return it*step + start

    def interp_R(self, start, end, it, Pass):
        return 1/self.interp_L(1/start, 1/end, it, Pass)
    
    def run(self):
        print("training...")
        writer = SummaryWriter('./tensorboard/' + self.timestamp)

        is_displacement_network_init = False 
        meshlabserver = Meshlabserver()
        
        remesh_stride = 30
        for epoch in range(self.start_epoch, self.nepochs + 1):
            
            if self.enable_remeshing and (self.train_explicit_vertices_original or self.train_explicit_vertices_large_step) and (epoch % remesh_stride) == 0:
                
                i_pass = epoch / remesh_stride
                pass_num = self.nepochs / remesh_stride
                
                remesh_len = self.interp_R(0.01, 0.005, i_pass, pass_num)
                curr_lr = self.interp_R(self.lr, self.lr * 0.5, i_pass, pass_num)
                print(f'remesh_len {remesh_len:g} curr_lr {curr_lr:g}')
                if self.model.displaced_sdf_mesh is None:
                    self.model.displaced_sdf_mesh = self.model.sdf_mesh
                self.model.sdf_mesh = meshlabserver.remesh(self.model.displaced_sdf_mesh, remesh_len)
                self.model.mesh_vertices_tensor = torch.from_numpy(np.array(self.model.sdf_mesh.vertices)).float().cuda() 
                if self.train_explicit_vertices_original:
                    self.explicit_vertices_tensor = self.model.mesh_vertices_tensor.clone().detach()  
                    self.explicit_vertices_tensor = self.explicit_vertices_tensor.cuda().requires_grad_()
                    self.optimizer = torch.optim.Adam([self.explicit_vertices_tensor], lr=curr_lr)
                if self.train_explicit_vertices_large_step:
                    self.explicit_vertices_tensor = self.model.mesh_vertices_tensor.clone()
                    self.mesh_faces_tensor = torch.from_numpy(self.model.sdf_mesh.faces).int().cuda().detach()
                    my_lambda = 10.0
                    self.M = compute_matrix(self.explicit_vertices_tensor, self.mesh_faces_tensor, my_lambda)
                    # Parameterize
                    self.explicit_vertices_tensor_u = to_differential(self.M, self.explicit_vertices_tensor)
                    self.explicit_vertices_tensor_u = self.explicit_vertices_tensor_u.requires_grad_()
                    step_size = 2e-3 # Step size
                    self.optimizer = AdamUniform([self.explicit_vertices_tensor_u], step_size)
        
            print("epoch", epoch) 

            if (self.train_displacements_single_mlp or self.train_displacements_multi_mlp) and not self.is_continue:
                if is_displacement_network_init == False or (epoch % 100000 == 0):
                # mesh_vertices_tensor, gt_displacement_values = self.model.extract_mesh_from_sdf()
                # print('extract_mesh_from_sdf') 
                # print(gt_displacement_values.shape)
                # print(mesh_vertices_tensor.shape)
                # if is_displacement_network_init == False:
                    # gt_displacement_values = torch.zeros_like(self.model.mesh_vertices_tensor)
                    # for i in range(10000):
                    for i in range(10000):
                        if self.train_displacements_multi_mlp:
                            displacement_values = self.model.get_displacement_value(self.model.mesh_vertices_tensor.detach(), is_multi_mlp=True)
                        else:
                            displacement_values = self.model.get_displacement_value(self.model.mesh_vertices_tensor.detach(), is_multi_mlp=False)
                        sampling_idx = torch.randperm(self.model.mesh_vertices_tensor.shape[0])[:self.model.mesh_vertices_tensor.shape[0]]
                        # loss = torch.norm(gt_displacement_values[sampling_idx, :] - displacement_values, p=2, dim=1).mean()
                        loss = torch.norm(displacement_values[sampling_idx, :], p=2, dim=1).mean()
                        self.optimizer_for_init.zero_grad()
                        loss.backward()
                        self.optimizer_for_init.step()
                        if i % 200 == 0:
                            print('{0}: loss = {1}'.format(i, loss.item()))
                    is_displacement_network_init = True

            if epoch % self.plot_freq == 0 and epoch >= 0:
                self.save_checkpoints(epoch)

            if epoch % self.plot_freq == 0 and epoch >= 0:

                self.model.eval()
                self.train_dataset.sampling_size = -1
                indices, self.model_input, self.ground_truth = next(iter(self.plot_dataloader))

                self.model_input["intrinsics"] = self.model_input["intrinsics"].cuda()
                self.model_input["uv"] = self.model_input["uv"].cuda()
                self.model_input['pose'] = self.model_input['pose'].cuda()
                self.model_input['explicit_vertices'] = None
                self.model_input['explicit_vertices_u_for_large_step'] = None
                self.model_input['is_mesh_sdf_single_mlp'] = False 
                self.model_input['is_idr_sdf_single_mlp'] = False 
                self.model_input['is_displacements_single_mlp'] = self.train_displacements_single_mlp
                self.model_input['is_displacements_multi_mlp'] = self.train_displacements_multi_mlp
                self.model_input['use_our_corr'] = self.use_our_corr
                self.model_input['material_ior'] = self.material_ior
                if self.train_explicit_vertices_original:
                    self.model_input['explicit_vertices'] = self.explicit_vertices_tensor 
                if self.train_explicit_vertices_large_step:
                    self.model_input['explicit_vertices_u_for_large_step'] = self.explicit_vertices_tensor_u
                    self.model_input['M'] = self.M

                if (self.train_mesh_sdf_single_mlp or self.train_idr_sdf_single_mlp):
                    self.model.extract_mesh_from_sdf(True)
                    self.model_input['iteration'] = 0
                    self.model_input['epoch'] = 0
                    split = utils.split_input(self.model_input, self.total_pixels)
                    res = []
                    for s in split:
                        out = self.model(s)
                        res.append({
                            'rgb_values': out['rgb_values'].detach(),
                            'rgb_values_mask': out['rgb_values_mask'].detach(),
                            # 'points': out['points'].detach(),
                            # 'object_mask': out['object_mask'].detach(),
                        })
                    model_outputs = None
                    batch_size = self.ground_truth['rgb'].shape[0]
                    model_outputs = utils.merge_output(res, self.total_pixels, batch_size)
                    plt.plot(self.model,
                            indices,
                            model_outputs,
                            self.model_input['pose'],
                            self.ground_truth['rgb'],
                            self.plots_dir,
                            epoch,
                            self.img_res,
                            **self.plot_conf,
                            is_train_vertices=False)
                    self.model.sdf_mesh.export('{0}/surface_{1}.ply'.format(self.plots_dir, epoch), 'ply')
                else:
                    self.model_input['iteration'] = 0
                    self.model_input['epoch'] = 0
                    split = utils.split_input(self.model_input, self.total_pixels)
                    res = []
                    for s in split:
                        out = self.model(s)
                        res.append({
                            'rgb_values': out['rgb_values'].detach(),
                            # 'network_object_mask': out['network_object_mask'].detach(),
                            # 'points': out['points'].detach(),
                            # 'object_mask': out['object_mask'].detach(),
                        })
                    model_outputs = None
                    batch_size = self.ground_truth['rgb'].shape[0]
                    model_outputs = utils.merge_output(res, self.total_pixels, batch_size)
                    plt.plot(self.model,
                            indices,
                            model_outputs,
                            self.model_input['pose'],
                            self.ground_truth['rgb'],
                            self.plots_dir,
                            epoch,
                            self.img_res,
                            **self.plot_conf,
                            is_train_vertices=False)
                    self.model.displaced_sdf_mesh.export('{0}/surface_{1}.ply'.format(self.plots_dir, epoch), 'ply')

                self.model.train()

            self.train_dataset.sampling_size = self.num_pixels

            color_loss = []
            corr_loss = []
            outside_border_loss = []
            DRT_ray_loss = []
            displaced_silhouette_loss = []
            displaced_normal_consistency_loss = []
            displaced_laplacian_smoothing_loss = []
            zero_displacement_loss = []
            eikonal_loss = []
            total_loss = [] 
            for data_index, (indices, self.model_input, self.ground_truth) in enumerate(self.train_dataloader):
                iteration = epoch * len(self.train_dataloader) + data_index

                self.model_input["intrinsics"] = self.model_input["intrinsics"].cuda()
                self.model_input["uv"] = self.model_input["uv"].cuda()
                self.model_input['pose'] = self.model_input['pose'].cuda()
                self.model_input['explicit_vertices'] = None
                self.model_input['explicit_vertices_u_for_large_step'] = None
                self.model_input['is_mesh_sdf_single_mlp'] = self.train_mesh_sdf_single_mlp 
                self.model_input['is_idr_sdf_single_mlp'] = self.train_idr_sdf_single_mlp 
                self.model_input['is_displacements_single_mlp'] = self.train_displacements_single_mlp
                self.model_input['is_displacements_multi_mlp'] = self.train_displacements_multi_mlp
                self.model_input['use_our_corr'] = self.use_our_corr
                self.model_input['material_ior'] = self.material_ior
                if self.train_explicit_vertices_original:
                    self.model_input['explicit_vertices'] = self.explicit_vertices_tensor 
                if self.train_explicit_vertices_large_step:
                    self.model_input['explicit_vertices_u_for_large_step'] = self.explicit_vertices_tensor_u
                    self.model_input['M'] = self.M

                self.model_input['iteration'] = iteration 
                self.model_input['epoch'] = epoch 

                model_outputs = self.model(idr_input=self.model_input)
                loss_output = self.loss(model_outputs, self.ground_truth, epoch)
                loss = loss_output['loss']

                self.optimizer.zero_grad()
                self.optimizer_ior.zero_grad()

                loss.backward()

                self.optimizer.step()
                # self.optimizer_ior.step(self.get_loss)

                print('{0} [{1}] ({2}/{3}), ior: {4}\n--------------------------------'\
                    .format(self.expname, epoch, data_index, self.n_batches, self.material_ior[0]))
            
                color_loss.append(loss_output['color_loss'].item())
                corr_loss.append(loss_output['corr_loss'].item())
                outside_border_loss.append(loss_output['outside_border_loss'].item())
                # DRT_ray_loss.append(loss_output['DRT_ray_loss'].item())
                displaced_silhouette_loss.append(loss_output['displaced_silhouette_loss'].item())
                displaced_normal_consistency_loss.append(loss_output['displaced_normal_consistency_loss'].item())
                displaced_laplacian_smoothing_loss.append(loss_output['displaced_laplacian_smoothing_loss'].item())
                eikonal_loss.append(loss_output['eikonal_loss'].item())
                total_loss.append(loss.item())

            writer.add_scalar('loss/color_loss', np.array(color_loss).mean(), epoch)
            writer.add_scalar('loss/corr_loss', np.array(corr_loss).mean(), epoch)
            writer.add_scalar('loss/outside_border_loss', np.array(outside_border_loss).mean(), epoch)
            # writer.add_scalar('loss/DRT_ray_loss', np.array(DRT_ray_loss).mean(), epoch)
            writer.add_scalar('loss/displaced_silhouette_loss', np.array(displaced_silhouette_loss).mean(), epoch)
            writer.add_scalar('loss/displaced_normal_consistency_loss', np.array(displaced_normal_consistency_loss).mean(), epoch)
            writer.add_scalar('loss/displaced_laplacian_smoothing_loss', np.array(displaced_laplacian_smoothing_loss).mean(), epoch)
            # writer.add_scalar('loss/displaced_edge_loss', np.array(displaced_edge_loss).mean(), epoch)
            # writer.add_scalar('loss/zero_displacement_loss', np.array(zero_displacement_loss).mean(), epoch)
            # writer.add_scalar('loss/vh_mesh_loss', np.array(vh_mesh_loss).mean(), epoch)
            writer.add_scalar('loss/eikonal_loss', np.array(eikonal_loss).mean(), epoch)
            writer.add_scalar('loss/total_loss', np.array(total_loss).mean(), epoch)
            writer.flush()

          
